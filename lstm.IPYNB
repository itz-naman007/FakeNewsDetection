{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba5aa534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from datasets import load_dataset, ClassLabel\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "658d5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2060af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELFake_Dataset.csv not found. Attempting to load 'davanstrien/WELFake' from Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Text Preprocessing Functions ---\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = load_dataset('csv', data_files='WELFake_Dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"WELFake_Dataset.csv not found. Attempting to load 'davanstrien/WELFake' from Hugging Face Hub.\")\n",
    "    dataset = load_dataset(\"davanstrien/WELFake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18ecd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'train' in dataset:\n",
    "    data_split = dataset['train']\n",
    "else:\n",
    "    data_split = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7216eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_text(example):\n",
    "    combined_text = \"\"\n",
    "    if 'title' in example and example['title'] is not None:\n",
    "        combined_text += str(example['title'])\n",
    "    if 'text' in example and example['text'] is not None:\n",
    "        if combined_text:\n",
    "            combined_text += \" \"\n",
    "        combined_text += str(example['text'])\n",
    "    \n",
    "    if not combined_text.strip():\n",
    "        return {\"full_text\": \"\"}\n",
    "    return {\"full_text\": combined_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c72ad2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_split = data_split.map(combine_text)\n",
    "data_split = data_split.remove_columns(['title', 'text']) if 'title' in data_split.column_names and 'text' in data_split.column_names else data_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d7e8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Keep alphanumeric characters and spaces only, then remove extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84ad7f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc0e802706c4c028009f881d684c1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_split = data_split.map(lambda example: {\"full_text\": clean_text(example[\"full_text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5bdfc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rename 'label' to 'labels' and cast to ClassLabel if not already\n",
    "if 'label' in data_split.column_names:\n",
    "    data_split = data_split.rename_column(\"label\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2353068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample preprocessed text: law enforcement on high alert following threats against cops and whites on 911by blacklivesmatter and fyf911 terrorists video no comment is expected from barack obama members of the fyf911 or fukyoflag and blacklivesmatter movements called for the lynching and hanging of white people and cops they encouraged others on a radio show tuesday night to turn the tide and kill white people and cops to send a message about the killing of black people in americaone of the fyoflag organizers is called sunshine she has a radio blog show hosted from texas called sunshine s fing opinion radio show a snapshot of her fyf911 lolatwhitefear twitter page at 953 pm shows that she was urging supporters to call now fyf911 tonight we continue to dismantle the illusion of white below is a snapshot twitter radio call invite fyf911the radio show aired at 1000 pm eastern standard timeduring the show callers clearly call for lynching and killing of white peoplea 239 minute clip from the radio show can be heard here it was provided to breitbart texas by someone who would like to be referred to as hannibal he has already received death threats as a result of interrupting fyf911 conference callsan unidentified black man said when those mother fkers are by themselves that s when when we should start fing them up like they do us when a bunch of them niers takin one of us out that s how we should roll up he said cause we already roll up in gangs anyway there should be six or seven black mother fckers see that white person and then lynch their ass let s turn the tables they conspired that if cops started losing people then there will be a state of emergency he speculated that one of two things would happen a bigass r s war or niers they are going to start backin up we are already getting killed out here so what the fk we got to lose sunshine could be heard saying yep that s true that s so fking true he said we need to turn the tables on them our kids are getting shot out here somebody needs to become a sacrifice on their sidehe said everybody ain t down for that st or whatever but like i say everybody has a different position of war he continued because they don t give a fk anyway he said again we might as well utilized them for that st and turn the tables on these ners he said that way we can start lookin like we ain t havin that many casualties and there can be more causalities on their side instead of ours they are out their killing black people black lives don t matter that s what those mother fkers so we got to make it matter to them find a mother fker that is alone snap his ass and then fin hang him from a damn tree take a picture of it and then send it to the mother fkers we just need one example and then people will start watchin this will turn the tables on st he said he said this will start a trickledown effect he said that when one white person is hung and then they are just flathanging that will start the trickledown effect he continued black people are good at starting trends he said that was how to get the upperhand another black man spoke up saying they needed to kill cops that are killing us the first black male said that will be the best method right there breitbart texas previously reported how sunshine was upset when racist white people infiltrated and disrupted one of her conference calls she subsequently released the phone number of one of the infiltrators the veteran immediately started receiving threatening callsone of the fyoflag movement supporters allegedly told a veteran who infiltrated their publicly posted conference call we are going to rape and gut your pregnant wife and your fing piece of sht unborn creature will be hung from a tree breitbart texas previously encountered sunshine at a sandra bland protest at the waller county jail in texas where she said all white people should be killed she told journalists and photographers you see this nappyass hair on my head that means i am one of those more militant negroes she said she was at the protest because these redneck motherfkers murdered sandra bland because she had nappy hair like me fyf911 black radicals say they will be holding the imperial powers that are actually responsible for the terrorist attacks on september 11th accountable on that day as reported by breitbart texas there are several websites and twitter handles for the movement palmetto star describes himself as one of the head organizers he said in a youtube video that supporters will be burning their symbols of the illusion of their superiority their false white supremacy like the american flag the british flag police uniforms and ku klux klan hoodssierra mcgrone or nocturnus libertus posted you too can help a young afrikan clean their a with the rag of oppression she posted two photos one that appears to be herself and a photo of a black man wiping their naked butts with the american flagfor entire story breitbart news\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Sample preprocessed text:\", data_split['full_text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e7c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Tokenization and Padding for Keras LSTM ---\n",
    "VOCAB_SIZE = 10000 # Max number of words to keep in vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 256 # Max length of text sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bbe963a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Keras Tokenizer and converting text to sequences...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nFitting Keras Tokenizer and converting text to sequences...\")\n",
    "# Initialize Keras Tokenizer\n",
    "# num_words: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
    "# oov_token: A token to represent out-of-vocabulary words.\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74325418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the tokenizer on the full_text column of the dataset\n",
    "# This builds the vocabulary\n",
    "tokenizer.fit_on_texts(data_split['full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f178d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(data_split['full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf449be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "023d052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare labels (one-hot encode for Keras categorical_crossentropy)\n",
    "labels = np.array(data_split['labels'])\n",
    "num_classes = len(np.unique(labels))\n",
    "labels_one_hot = to_categorical(labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7ff0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data shape: (57707, 256)\n",
      "Training labels shape: (57707, 2)\n",
      "Test data shape: (14427, 256)\n",
      "Test labels shape: (14427, 2)\n",
      "Vocabulary size after tokenization: 352710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Split the Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences, labels_one_hot, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Vocabulary size after tokenization: {len(tokenizer.word_index) + 1}\") # +1 for padding/Oov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5bff7f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naman\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Keras LSTM Model Definition ---\n",
    "EMBEDDING_DIM = 128 # Dimension of the word embeddings\n",
    "LSTM_UNITS = 128    # Number of units in the LSTM layer\n",
    "DROPOUT_RATE = 0.3  # Dropout rate for regularization\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding layer: vocab_size + 1 because Keras tokenizer reserves index 0 for padding.\n",
    "    Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    # Bidirectional LSTM processes sequence in both directions\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False)), # return_sequences=False for sequence-to-one classification\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(num_classes, activation='softmax') # Output layer with softmax for classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cb65d0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "# Using 'adam' optimizer and 'categorical_crossentropy' for one-hot encoded labels\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17e3fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Training ---\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ef5647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs with batch size 32...\n",
      "Epoch 1/10\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.9064 - loss: 0.2109\n",
      "Epoch 1: val_loss improved from inf to 0.08090, saving model to best_lstm_model_tf.keras\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 254ms/step - accuracy: 0.9064 - loss: 0.2109 - val_accuracy: 0.9702 - val_loss: 0.0809\n",
      "Epoch 2/10\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.9803 - loss: 0.0593\n",
      "Epoch 2: val_loss improved from 0.08090 to 0.07644, saving model to best_lstm_model_tf.keras\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 263ms/step - accuracy: 0.9803 - loss: 0.0593 - val_accuracy: 0.9710 - val_loss: 0.0764\n",
      "Epoch 3/10\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9920 - loss: 0.0251\n",
      "Epoch 3: val_loss did not improve from 0.07644\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 273ms/step - accuracy: 0.9920 - loss: 0.0251 - val_accuracy: 0.9704 - val_loss: 0.0921\n",
      "Epoch 4/10\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.9959 - loss: 0.0129\n",
      "Epoch 4: val_loss did not improve from 0.07644\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 253ms/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9756 - val_loss: 0.0880\n",
      "Epoch 5/10\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.9986 - loss: 0.0052\n",
      "Epoch 5: val_loss did not improve from 0.07644\n",
      "\u001b[1m1804/1804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 255ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 0.9744 - val_loss: 0.0987\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Callbacks for early stopping and saving the best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_lstm_model_tf.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs with batch size {BATCH_SIZE}...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test), # Using test set as validation for simplicity\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "54af8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model on Test Set ---\n",
      "\n",
      "Final Test Loss: 0.0764\n",
      "Final Test Accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Model Evaluation ---\n",
    "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
    "# Load the best saved model for final evaluation\n",
    "best_model = tf.keras.models.load_model('best_lstm_model_tf.keras')\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nFinal Test Loss: {loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fbdb9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.99      0.95      0.97      7006\n",
      "        fake       0.96      0.99      0.97      7421\n",
      "\n",
      "    accuracy                           0.97     14427\n",
      "   macro avg       0.97      0.97      0.97     14427\n",
      "weighted avg       0.97      0.97      0.97     14427\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6670  336]\n",
      " [  83 7338]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detailed Classification Report\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['real', 'fake']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75a46edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_lstm_tf_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
