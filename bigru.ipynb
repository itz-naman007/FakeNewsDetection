{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0afca2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naman\\anaconda3\\envs\\ml-dl-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout # Changed LSTM to GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478d50d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELFake_Dataset.csv not found. Attempting to load 'davanstrien/WELFake' from Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Text Preprocessing Functions ---\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = load_dataset('csv', data_files='WELFake_Dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"WELFake_Dataset.csv not found. Attempting to load 'davanstrien/WELFake' from Hugging Face Hub.\")\n",
    "    dataset = load_dataset(\"davanstrien/WELFake\")\n",
    "\n",
    "if 'train' in dataset:\n",
    "    data_split = dataset['train']\n",
    "else:\n",
    "    data_split = dataset\n",
    "\n",
    "def combine_text(example):\n",
    "    combined_text = \"\"\n",
    "    if 'title' in example and example['title'] is not None:\n",
    "        combined_text += str(example['title'])\n",
    "    if 'text' in example and example['text'] is not None:\n",
    "        if combined_text:\n",
    "            combined_text += \" \"\n",
    "        combined_text += str(example['text'])\n",
    "    \n",
    "    if not combined_text.strip():\n",
    "        return {\"full_text\": \"\"}\n",
    "    return {\"full_text\": combined_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6595a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_split = data_split.map(combine_text)\n",
    "data_split = data_split.remove_columns(['title', 'text']) if 'title' in data_split.column_names and 'text' in data_split.column_names else data_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53469bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff80609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_split = data_split.map(lambda example: {\"full_text\": clean_text(example[\"full_text\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e24de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample preprocessed text: law enforcement on high alert following threats against cops and whites on 911by blacklivesmatter and fyf911 terrorists video no comment is expected from barack obama members of the fyf911 or fukyoflag and blacklivesmatter movements called for the lynching and hanging of white people and cops they encouraged others on a radio show tuesday night to turn the tide and kill white people and cops to send a message about the killing of black people in americaone of the fyoflag organizers is called sunshine she has a radio blog show hosted from texas called sunshine s fing opinion radio show a snapshot of her fyf911 lolatwhitefear twitter page at 953 pm shows that she was urging supporters to call now fyf911 tonight we continue to dismantle the illusion of white below is a snapshot twitter radio call invite fyf911the radio show aired at 1000 pm eastern standard timeduring the show callers clearly call for lynching and killing of white peoplea 239 minute clip from the radio show can be heard here it was provided to breitbart texas by someone who would like to be referred to as hannibal he has already received death threats as a result of interrupting fyf911 conference callsan unidentified black man said when those mother fkers are by themselves that s when when we should start fing them up like they do us when a bunch of them niers takin one of us out that s how we should roll up he said cause we already roll up in gangs anyway there should be six or seven black mother fckers see that white person and then lynch their ass let s turn the tables they conspired that if cops started losing people then there will be a state of emergency he speculated that one of two things would happen a bigass r s war or niers they are going to start backin up we are already getting killed out here so what the fk we got to lose sunshine could be heard saying yep that s true that s so fking true he said we need to turn the tables on them our kids are getting shot out here somebody needs to become a sacrifice on their sidehe said everybody ain t down for that st or whatever but like i say everybody has a different position of war he continued because they don t give a fk anyway he said again we might as well utilized them for that st and turn the tables on these ners he said that way we can start lookin like we ain t havin that many casualties and there can be more causalities on their side instead of ours they are out their killing black people black lives don t matter that s what those mother fkers so we got to make it matter to them find a mother fker that is alone snap his ass and then fin hang him from a damn tree take a picture of it and then send it to the mother fkers we just need one example and then people will start watchin this will turn the tables on st he said he said this will start a trickledown effect he said that when one white person is hung and then they are just flathanging that will start the trickledown effect he continued black people are good at starting trends he said that was how to get the upperhand another black man spoke up saying they needed to kill cops that are killing us the first black male said that will be the best method right there breitbart texas previously reported how sunshine was upset when racist white people infiltrated and disrupted one of her conference calls she subsequently released the phone number of one of the infiltrators the veteran immediately started receiving threatening callsone of the fyoflag movement supporters allegedly told a veteran who infiltrated their publicly posted conference call we are going to rape and gut your pregnant wife and your fing piece of sht unborn creature will be hung from a tree breitbart texas previously encountered sunshine at a sandra bland protest at the waller county jail in texas where she said all white people should be killed she told journalists and photographers you see this nappyass hair on my head that means i am one of those more militant negroes she said she was at the protest because these redneck motherfkers murdered sandra bland because she had nappy hair like me fyf911 black radicals say they will be holding the imperial powers that are actually responsible for the terrorist attacks on september 11th accountable on that day as reported by breitbart texas there are several websites and twitter handles for the movement palmetto star describes himself as one of the head organizers he said in a youtube video that supporters will be burning their symbols of the illusion of their superiority their false white supremacy like the american flag the british flag police uniforms and ku klux klan hoodssierra mcgrone or nocturnus libertus posted you too can help a young afrikan clean their a with the rag of oppression she posted two photos one that appears to be herself and a photo of a black man wiping their naked butts with the american flagfor entire story breitbart news\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'label' in data_split.column_names:\n",
    "    data_split = data_split.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(\"Sample preprocessed text:\", data_split['full_text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012bc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dataset Splitting ---\n",
    "train_test_split = data_split.train_test_split(test_size=0.2, stratify_by_column=\"labels\", seed=42)\n",
    "\n",
    "train_texts = [ex['full_text'] for ex in train_test_split['train']]\n",
    "train_labels = np.array([ex['labels'] for ex in train_test_split['train']], dtype=np.int32)\n",
    "\n",
    "test_texts = [ex['full_text'] for ex in train_test_split['test']]\n",
    "test_labels = np.array([ex['labels'] for ex in train_test_split['test']], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c31bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building vocabulary with TextVectorization...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Keras TextVectorization and Padding ---\n",
    "MAX_TOKENS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "print(\"\\nBuilding vocabulary with TextVectorization...\")\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    standardize=None \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2805f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "Sample vocabulary: ['', '[UNK]', 'the', 'to', 'of', 'and', 'a', 'in', 'that', 'is', 'for', 'on', 'it', 'he', 'with', 'was', 'as', 'said', 'by', 'trump']\n",
      "\n",
      "Train Sequences Example (first 5 values): [3145 3674    1  124  902]\n",
      "Train Labels Example: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "VOCAB_SIZE = len(vectorize_layer.get_vocabulary())\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(\"Sample vocabulary:\", vectorize_layer.get_vocabulary()[:20])\n",
    "\n",
    "train_sequences = vectorize_layer(tf.constant(train_texts)).numpy()\n",
    "test_sequences = vectorize_layer(tf.constant(test_texts)).numpy()\n",
    "\n",
    "print(\"\\nTrain Sequences Example (first 5 values):\", train_sequences[0][:5])\n",
    "print(\"Train Labels Example:\", train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a665167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naman\\anaconda3\\envs\\ml-dl-env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- BiGRU Model Definition (TensorFlow Keras) ---\n",
    "EMBEDDING_DIM = 100\n",
    "GRU_UNITS = 256 # Units for the GRU layer\n",
    "DROPOUT_RATE = 0.5\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,)),\n",
    "    Embedding(input_dim=VOCAB_SIZE, \n",
    "              output_dim=EMBEDDING_DIM,\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              mask_zero=True),\n",
    "    \n",
    "    # Using Bidirectional wrapper around GRU layer\n",
    "    Bidirectional(GRU(GRU_UNITS, return_sequences=False)), \n",
    "    \n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9268ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">549,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m549,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,550,914</span> (5.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,550,914\u001b[0m (5.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,550,914</span> (5.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,550,914\u001b[0m (5.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Compile the Model ---\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c97577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Callbacks ---\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_bigru_tf_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f636b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs with batch size 64...\n",
      "Epoch 1/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 846ms/step - accuracy: 0.8534 - loss: 0.3118 - val_accuracy: 0.9792 - val_loss: 0.0601\n",
      "Epoch 2/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 754ms/step - accuracy: 0.9862 - loss: 0.0421 - val_accuracy: 0.9781 - val_loss: 0.0615\n",
      "Epoch 3/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 777ms/step - accuracy: 0.9951 - loss: 0.0153 - val_accuracy: 0.9812 - val_loss: 0.0551\n",
      "Epoch 4/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 901ms/step - accuracy: 0.9976 - loss: 0.0084 - val_accuracy: 0.9832 - val_loss: 0.0747\n",
      "Epoch 5/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 622ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 0.9816 - val_loss: 0.0840\n",
      "Epoch 6/10\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 768ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 0.9825 - val_loss: 0.0787\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train the Model ---\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs with batch size {BATCH_SIZE}...\")\n",
    "history = model.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(test_sequences, test_labels),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "print(\"\\n--- Training Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d6771c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model on Test Set ---\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 155ms/step - accuracy: 0.9821 - loss: 0.0504\n",
      "Test Loss: 0.0551\n",
      "Test Accuracy: 98.12%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Evaluate the Model on the Test Set ---\n",
    "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
    "loss, accuracy = model.evaluate(test_sequences, test_labels, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372c6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "333323ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comprehensive Classification Report ---\n",
      "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 123ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Generate Comprehensive Classification Report ---\n",
    "print(\"\\n--- Comprehensive Classification Report ---\")\n",
    "# Get raw predictions (probabilities)\n",
    "predictions_raw = model.predict(test_sequences)\n",
    "# Convert probabilities to predicted class (0 or 1)\n",
    "predicted_classes = np.argmax(predictions_raw, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a6b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.98      0.98      0.98      7006\n",
      "        fake       0.98      0.98      0.98      7421\n",
      "\n",
      "    accuracy                           0.98     14427\n",
      "   macro avg       0.98      0.98      0.98     14427\n",
      "weighted avg       0.98      0.98      0.98     14427\n",
      "\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[6892  114]\n",
      " [ 157 7264]]\n"
     ]
    }
   ],
   "source": [
    "# Define target names based on your numerical labels (0 and 1)\n",
    "# Assuming 0: real, 1: fake, based on your previous reports\n",
    "target_names = ['real', 'fake']\n",
    "\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "print(confusion_matrix(test_labels, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f3417bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"bigru.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
